{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI316 Individual Assignment 1\n",
    "# Thawdar Swe Zin (8039276)\n",
    "## (Task 2)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "import math\n",
    "from pprint import pprint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframes\n",
    "churnTest_df = pd.read_csv(\"customer_churn_dataset-testing-master.csv\")\n",
    "churnTrain_df = pd.read_csv(\"customer_churn_dataset-training-master.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64374"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(churnTest_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "440833"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(churnTrain_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Implement three DT models by the split criteria of Information Gain, Gain Ratio and Gini Index. You can use either binary-split or multiple-split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will be using binary splits, ie: == or != statements, we will need a function to determine the optimal partioning\n",
    "\n",
    "The following functions are implemented to achieve our objectives: \n",
    "\n",
    "    1. Function for calculating information gain, gini index,and gain ratio\n",
    "    2. Function for checking if we have reached a leaf node (stopping criteria)\n",
    "    3. Function for grabbing the class one we have reached a leaf node\n",
    "    4. Function for getting all possible splits\n",
    "    5. (3) functions for determining best split (info gain, gain index, gain ratio)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function for calculating information gain, gini index,and gain ratio  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of dataset:  0.9870102753005665\n"
     ]
    }
   ],
   "source": [
    "# calculate entropy for the dataset (for testing/ verification)\n",
    "label_col = churnTrain_df.iloc[:,-1]\n",
    "_, n_counts = np.unique(label_col, return_counts=True)\n",
    "\n",
    "prob = n_counts / n_counts.sum()\n",
    "entropyOfDataset = sum(prob * -np.log2(prob))\n",
    "\n",
    "print(\"Entropy of dataset: \", entropyOfDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for binary splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, split_col, split_val):\n",
    "    split_col_val = data[:, split_col]\n",
    "\n",
    "    left = data[split_col_val == split_val]\n",
    "    right = data[split_col_val != split_val]\n",
    "\n",
    "    return left, right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for calculating best split based on information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(data):\n",
    "    # calculate entropy for the dataset (for testing/ verification)\n",
    "    label_col = churnTrain_df.iloc[:,-1]\n",
    "    _, n_counts = np.unique(label_col, return_counts=True)\n",
    "\n",
    "    prob = n_counts / n_counts.sum()\n",
    "    entropyOfDataset = sum(prob * -np.log2(prob))\n",
    "\n",
    "    return entropyOfDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_child_entropy(left, right):\n",
    "    # calculate total number of rows\n",
    "    n = len(left) + len(right)\n",
    "\n",
    "    # calculate the occurences of each feature\n",
    "    p_left =len(left) / n\n",
    "    p_right = len(right) / n\n",
    "\n",
    "\n",
    "    # calculate weighted entropy\n",
    "    child_entropy = (p_left * calculate_child_entropy(left) + p_right * calculate_entropy(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_best_split(data, possible_splits):\n",
    "    \n",
    "    # calculate the entropy of the data\n",
    "    parent_entropy = calculate_entropy(data)\n",
    "    best_entropy = parent_entropy\n",
    "\n",
    "    # loop through all values for each column \n",
    "    for col_index in possible_splits:\n",
    "        for value in possible_splits[col_index]:\n",
    "            left, right = split_data(data, split_col=col_index, split_val=value)\n",
    "            child_entropy = calculate_child_entropy(left, right)\n",
    "\n",
    "            #c compare each entropy value and find lowest\n",
    "            if child_entropy < best_entropy:\n",
    "                best_split_col = col_index\n",
    "                best_split_val = value\n",
    "                best_entropy = child_entropy\n",
    "                info_gain = parent_entropy - best_entropy\n",
    "\n",
    "    return best_split_col, best_split_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for calculating best split based on gini index<lowest>\n",
    "\n",
    "    1. calculate gini index and gini gain for each feature\n",
    "    2. compare gini gain in each feature\n",
    "    3. feature with highest gini gain is selected as the splitting feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gini(data):\n",
    "    # calculate gini for target\n",
    "    class_labels = np.unique(data)\n",
    "\n",
    "    gini = 0\n",
    "\n",
    "    for classes in class_labels:\n",
    "        p_classes = len(data[data == classes]) / len(data)\n",
    "        gini += p_classes ** 2\n",
    "\n",
    "    return 1 - gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gini_gain(parent, left, right):\n",
    "    left_weight = len(left) / len(parent)\n",
    "    right_weight = len(right) / len(parent)\n",
    "\n",
    "    giniGain = calculate_gini(parent) - (left_weight* calculate_gini(left) + right_weight * calculate_gini(right))\n",
    "\n",
    "    return giniGain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_best_split2(data, possible_splits):\n",
    "    label_col = data[:, -1]\n",
    "\n",
    "    # initialise maz gini for comparision\n",
    "    max_gini_gain = -float(\"inf\")\n",
    "\n",
    "    #dictionary to store possible splits and corresponding gini index\n",
    "    gini = {}\n",
    "\n",
    "    # loop through all values for each column \n",
    "    for col_index in possible_splits:\n",
    "        for value in possible_splits[col_index]:\n",
    "            left, right = split_data(data, split_col = col_index, split_val=value)\n",
    "            if len(left) > 0 and len(right) > 0:\n",
    "                y, left_y, right_y = data[:, -1], left[:, -1], right[:, -1]\n",
    "                # calculate gini for each feature subset\n",
    "                curr_gini_gain = calculate_gini_gain(y, left_y, right_y)\n",
    "\n",
    "                # compare each subset's value\n",
    "                if curr_gini_gain > max_gini_gain:\n",
    "                    max_gini_gain = curr_gini_gain\n",
    "                    best_split_col = col_index\n",
    "                    best_split_val = value\n",
    "    return best_split_col, best_split_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for calculation best split based on gain ratio \n",
    "\n",
    "    1. calculate information gain and split info for each feature\n",
    "    2. compare gain ratio in each feature\n",
    "    3. feature with highest gain ratio is seltected as the splitting feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate split information for one feature\n",
    "def calculate_Split_info(data_col):\n",
    "    split_info = 0\n",
    "\n",
    "    unique_vals, n_counts = np.unique(data_col, return_counts=True)\n",
    "    prob = n_counts/ n_counts.sum()\n",
    "\n",
    "    for idx, values in enumerate(unique_vals):\n",
    "\n",
    "        occurences = n_counts[idx]\n",
    "        prob = occurences/ n_counts.sum()\n",
    "        temp = (prob* - np.log2(prob))\n",
    "        split_info = split_info + temp\n",
    "    \n",
    "    return(split_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.00000e+00 3.00000e+00 4.00000e+00 ... 4.49998e+05 4.49999e+05\n",
      "         nan]\n",
      "[1 1 1 ... 1 1 1]\n",
      "440833\n",
      "18.749872700174883\n"
     ]
    }
   ],
   "source": [
    "split_info = 0\n",
    "\n",
    "unique_vals, n_counts = np.unique(churnTrain_df['CustomerID'], return_counts=True)\n",
    "prob = n_counts/ n_counts.sum()\n",
    "\n",
    "print(unique_vals)\n",
    "print(n_counts)\n",
    "print(n_counts.sum())\n",
    "\n",
    "for idx, values in enumerate(unique_vals):\n",
    "\n",
    "    occurences = n_counts[idx]\n",
    "    prob = occurences/ n_counts.sum()\n",
    "    temp = (prob* - np.log2(prob))\n",
    "    split_info = split_info + temp\n",
    "\n",
    "print(split_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings:**\n",
    "\n",
    ">As all data are shown in 1, no duplicated data is found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for checking if we have reached a leaf node (tunning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_leaf(data):\n",
    "    ## store last column as label_col\n",
    "    label_col = data[:, -1]\n",
    "\n",
    "    # use np.unique() to extract out unique values from label_col\n",
    "    classes = np.unique(label_col)\n",
    "\n",
    "    if len(classes) == 1:\n",
    "        return True\n",
    "    else: \n",
    "        return False; \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.0, 30.0, 'Female', 39.0, 14.0, 5.0, 18.0, 'Standard', 'Annual',\n",
       "        932.0, 17.0, 1.0],\n",
       "       [3.0, 65.0, 'Female', 49.0, 1.0, 10.0, 8.0, 'Basic', 'Monthly',\n",
       "        557.0, 6.0, 1.0],\n",
       "       [4.0, 55.0, 'Female', 14.0, 4.0, 6.0, 18.0, 'Basic', 'Quarterly',\n",
       "        185.0, 3.0, 1.0],\n",
       "       [5.0, 58.0, 'Male', 38.0, 21.0, 7.0, 7.0, 'Standard', 'Monthly',\n",
       "        396.0, 29.0, 1.0],\n",
       "       [6.0, 23.0, 'Male', 32.0, 20.0, 5.0, 8.0, 'Basic', 'Monthly',\n",
       "        617.0, 20.0, 1.0]], dtype=object)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_copy = churnTrain_df.values\n",
    "data_copy[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1], dtype=int64), array([33881, 30493], dtype=int64))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(churnTest_df[\"Churn\"], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_leaf(churnTrain_df.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for Classification\n",
    "\n",
    "Function gets the class for when a leaf node is reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_data(data):\n",
    "    # since our labels are always in the last column \n",
    "    label_col = data[:, -1]\n",
    "   \n",
    "    # find each unique classes in the label column and how many times it appears\n",
    "    classes, n_classes = np.unique(label_col, return_counts=True)\n",
    "\n",
    "    # find index of the most frequent class\n",
    "    index = n_classes.argmax()\n",
    "\n",
    "    # extract out the name \n",
    "    classification = classes[index]\n",
    "\n",
    "    return classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_data(churnTrain_df.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding:\n",
    "\n",
    "According to the above analysis, the most frequent data of last label column is '0.0'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree Classifier procedure\n",
    "\n",
    "purning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_splits(data):\n",
    "    possible_splits = {}\n",
    "    n_rows, n_cols = data.shape\n",
    "    print(\"data shape:\", data.shape)\n",
    "    print(\"range(n_cols-1)\", range(n_cols-1))\n",
    "    for col_index in range(n_cols-1):\n",
    "        possible_splits[col_index] = {}\n",
    "        values = data[:, col_index]\n",
    "        print(\"Values:\", values)\n",
    "        unique_values = np.unique(values)\n",
    "\n",
    "        possible_splits[col_index] = unique_values\n",
    "\n",
    "    return possible_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_classifier(df, max_depth, min_samples, counter = 0):\n",
    "\n",
    "    # initialize variables at the start\n",
    "    if counter == 0:\n",
    "        global COLUMN_HEADERS\n",
    "        COLUMN_HEADERS = df.columns\n",
    "        data= df.values\n",
    "    else: \n",
    "        data = df\n",
    "\n",
    "    print(len(data))\n",
    "\n",
    "    # return data\n",
    "    # early stopping conditions \n",
    "    if(check_leaf(data)) or (len(data) < min_samples) or (counter == max_depth):\n",
    "        classification = classify_data(data)\n",
    "        return classification\n",
    "    else: \n",
    "        \n",
    "        counter += 1\n",
    "        possible_splits = get_possible_splits(data)\n",
    "\n",
    "        # use determine_best_split which returns the feature/col with best info gain\n",
    "        split_column, split_value = determine_best_split(data, possible_splits)\n",
    "\n",
    "        #split the data according to the result in determine_best_split\n",
    "        left, right = split_data(data, split_column, split_value)\n",
    "\n",
    "        # choose question \n",
    "        feature_name = COLUMN_HEADERS[split_column]\n",
    "\n",
    "        # format question \n",
    "        question = \"{} = {}\".format(feature_name, split_value)\n",
    "\n",
    "        # initialise dictionary for storing tree\n",
    "        # method will return this dictionary \n",
    "        sub_tree = {question: []}\n",
    "\n",
    "        # recursive call with left and right branches\n",
    "        yes_answer = decision_tree_classifier(left, max_depth, min_samples, counter)\n",
    "        no_answer = decision_tree_classifier(right, max_depth, min_samples, counter)\n",
    "\n",
    "        if yes_answer == no_answer:\n",
    "            sub_tree = yes_answer\n",
    "        else:\n",
    "            sub_tree[question].append(yes_answer)\n",
    "            sub_tree[question].append(no_answer)\n",
    "\n",
    "        return sub_tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Function for predicting values using the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_one (test, tree):\n",
    "\n",
    "    # save tree patern into list\n",
    "    question = list(tree.keys())[0]\n",
    "\n",
    "    # extract out feature label, operator'=' and values of the feature\n",
    "    # using split\n",
    "    feature_name, operator, value = question.split(\" \")\n",
    "\n",
    "    # asking the question \n",
    "    if str(test[feature_name]) == value: \n",
    "        answer = tree[question][0]\n",
    "    else:\n",
    "        answer = tree[question][1]\n",
    "\n",
    "    \n",
    "    # if it is a dictionary, it means that the answer is a subtree.\n",
    "    # then recursively call method until answer is found \n",
    "    # else, it is the answer\n",
    "    # then return answer\n",
    "\n",
    "    if not isinstance(answer, dict):\n",
    "        return answer\n",
    "    else:\n",
    "        residual_tree = answer\n",
    "        return classify_one(test, residual_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test, tree):\n",
    "\n",
    "    # initilaise array for storing \n",
    "    output = []\n",
    "    index = test.index.tolist()\n",
    "    for i in index:\n",
    "        output.append(classify_one(test.loc[i], tree))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function for calculating prediction accuracy\n",
    "\n",
    "By comparing all predictions to its actual values, calculating the average mean of total predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(df, tree): \n",
    "    df[\"classification\"] = df.apply(classify_one, axis=1, args=(tree,))\n",
    "    df[\"classification correct\"] = df[\"classification\"] == df[\"Class\"]\n",
    "\n",
    "    accuracy = df[\"classification_correct\"].mean()\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test out all the three models:\n",
    "\n",
    "##### Test out first model - decision_tree_classifier()\n",
    "        - split by info gain and entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = decision_tree_classifier(churnTrain_df, max_depth=10, min_samples=10)\n",
    "print(tree)\n",
    "output = predict(churnTest_df, tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
