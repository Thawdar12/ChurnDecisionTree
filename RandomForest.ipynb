{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o20ws7_V33Vj"
      },
      "source": [
        "\n",
        "# Task 1 : random forest model\n",
        "Thawdar Swe Zin (8039276)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXX3j3ly36ew"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QM5xzkcB4c-I"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEEynKr84g2u"
      },
      "source": [
        "## Read in data file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8Q-AlBI4k6_",
        "outputId": "ef6fb3b2-9474-45ec-df01-6b4c104593f8"
      },
      "outputs": [],
      "source": [
        "# Reading in dataset and Creating dataframes\n",
        "churnTest_df = pd.read_csv(\"customer_churn_dataset-testing-master.csv\")\n",
        "churnTrain_df = pd.read_csv(\"customer_churn_dataset-training-master.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTcC42j1Zd8u"
      },
      "source": [
        "## Data Preproccessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXk-3dhyZk1J"
      },
      "source": [
        "### Cleaning null values and unwanted columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dP5NRKfaZzPp"
      },
      "outputs": [],
      "source": [
        "churnTrain_df.dropna(how='any', inplace=True)\n",
        "churnTrain_df = churnTrain_df.iloc[: , 1:]\n",
        "\n",
        "# There is no NA in test dataset\n",
        "churnTest_df = churnTest_df.iloc[: , 1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 440832 entries, 0 to 440832\n",
            "Data columns (total 11 columns):\n",
            " #   Column             Non-Null Count   Dtype  \n",
            "---  ------             --------------   -----  \n",
            " 0   Age                440832 non-null  float64\n",
            " 1   Gender             440832 non-null  object \n",
            " 2   Tenure             440832 non-null  float64\n",
            " 3   Usage Frequency    440832 non-null  float64\n",
            " 4   Support Calls      440832 non-null  float64\n",
            " 5   Payment Delay      440832 non-null  float64\n",
            " 6   Subscription Type  440832 non-null  object \n",
            " 7   Contract Length    440832 non-null  object \n",
            " 8   Total Spend        440832 non-null  float64\n",
            " 9   Last Interaction   440832 non-null  float64\n",
            " 10  Churn              440832 non-null  float64\n",
            "dtypes: float64(8), object(3)\n",
            "memory usage: 40.4+ MB\n"
          ]
        }
      ],
      "source": [
        "churnTrain_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 64374 entries, 0 to 64373\n",
            "Data columns (total 11 columns):\n",
            " #   Column             Non-Null Count  Dtype \n",
            "---  ------             --------------  ----- \n",
            " 0   Age                64374 non-null  int64 \n",
            " 1   Gender             64374 non-null  object\n",
            " 2   Tenure             64374 non-null  int64 \n",
            " 3   Usage Frequency    64374 non-null  int64 \n",
            " 4   Support Calls      64374 non-null  int64 \n",
            " 5   Payment Delay      64374 non-null  int64 \n",
            " 6   Subscription Type  64374 non-null  object\n",
            " 7   Contract Length    64374 non-null  object\n",
            " 8   Total Spend        64374 non-null  int64 \n",
            " 9   Last Interaction   64374 non-null  int64 \n",
            " 10  Churn              64374 non-null  int64 \n",
            "dtypes: int64(8), object(3)\n",
            "memory usage: 5.4+ MB\n"
          ]
        }
      ],
      "source": [
        "churnTest_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIR8Rw_IZ2OP"
      },
      "source": [
        "### Converting category to numeric "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jqA0YcnqZ-Ju"
      },
      "outputs": [],
      "source": [
        "# Converting category to binary\n",
        "contract_categories = churnTrain_df['Contract Length'].unique()\n",
        "sub_type = churnTrain_df['Subscription Type'].unique()\n",
        "Age_categories = churnTrain_df['Gender'].unique()\n",
        "\n",
        "churnTrain_df['Contract Length'].replace(contract_categories,\n",
        "                        [0, 1 , 2], inplace=True)\n",
        "churnTrain_df['Subscription Type'].replace(sub_type,\n",
        "                        [0 , 1 , 2], inplace=True)\n",
        "churnTrain_df['Gender'].replace(Age_categories,\n",
        "                        [0 , 1], inplace=True)\n",
        "\n",
        "\n",
        "churnTest_df['Contract Length'].replace(contract_categories,\n",
        "                        [0, 1 , 2], inplace=True)\n",
        "churnTest_df['Subscription Type'].replace(sub_type,\n",
        "                        [0 , 1 , 2], inplace=True)\n",
        "churnTest_df['Gender'].replace(Age_categories,\n",
        "                        [0 , 1], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJNC6joaaCFu"
      },
      "source": [
        "### Creating a bin range and labels for continuous variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1gsumOmXaJaW"
      },
      "outputs": [],
      "source": [
        "attribute_to_bin = 'Total Spend' \n",
        "# Age is a continuous variable. So, we will be using it.\n",
        "# Hot encoding is not done after binning because random forest models are non-linear.\n",
        "\n",
        "churnTrain_df['Total Spend Bin'] = pd.qcut(churnTrain_df['Total Spend'], 5, labels=False) + 1\n",
        "churnTrain_df['Age Bin'] = pd.qcut(churnTrain_df['Age'], 5, labels=False) + 1\n",
        "\n",
        "churnTest_df['Total Spend Bin'] = pd.qcut(churnTrain_df['Total Spend'], 5, labels=False) + 1\n",
        "churnTest_df['Age Bin'] = pd.qcut(churnTrain_df['Age'], 5, labels=False) + 1\n",
        "\n",
        "# Drop initial column after binning\n",
        "churnTrain_df.drop('Total Spend', axis=1, inplace=True)\n",
        "churnTrain_df.drop('Age', axis=1, inplace=True)\n",
        "\n",
        "churnTest_df.drop('Total Spend', axis=1, inplace=True)\n",
        "churnTest_df.drop('Age', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 440832 entries, 0 to 440832\n",
            "Data columns (total 11 columns):\n",
            " #   Column             Non-Null Count   Dtype  \n",
            "---  ------             --------------   -----  \n",
            " 0   Gender             440832 non-null  int64  \n",
            " 1   Tenure             440832 non-null  float64\n",
            " 2   Usage Frequency    440832 non-null  float64\n",
            " 3   Support Calls      440832 non-null  float64\n",
            " 4   Payment Delay      440832 non-null  float64\n",
            " 5   Subscription Type  440832 non-null  int64  \n",
            " 6   Contract Length    440832 non-null  int64  \n",
            " 7   Last Interaction   440832 non-null  float64\n",
            " 8   Churn              440832 non-null  float64\n",
            " 9   Total Spend Bin    440832 non-null  int64  \n",
            " 10  Age Bin            440832 non-null  int64  \n",
            "dtypes: float64(6), int64(5)\n",
            "memory usage: 56.5 MB\n"
          ]
        }
      ],
      "source": [
        "churnTrain_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 64374 entries, 0 to 64373\n",
            "Data columns (total 11 columns):\n",
            " #   Column             Non-Null Count  Dtype\n",
            "---  ------             --------------  -----\n",
            " 0   Gender             64374 non-null  int64\n",
            " 1   Tenure             64374 non-null  int64\n",
            " 2   Usage Frequency    64374 non-null  int64\n",
            " 3   Support Calls      64374 non-null  int64\n",
            " 4   Payment Delay      64374 non-null  int64\n",
            " 5   Subscription Type  64374 non-null  int64\n",
            " 6   Contract Length    64374 non-null  int64\n",
            " 7   Last Interaction   64374 non-null  int64\n",
            " 8   Churn              64374 non-null  int64\n",
            " 9   Total Spend Bin    64374 non-null  int64\n",
            " 10  Age Bin            64374 non-null  int64\n",
            "dtypes: int64(11)\n",
            "memory usage: 5.4 MB\n"
          ]
        }
      ],
      "source": [
        "churnTest_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfNcz2i1aL2D"
      },
      "source": [
        "### Convert float to int for **standardlization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xCFW7bFJaU84"
      },
      "outputs": [],
      "source": [
        "# Function to convert float dtype to integers dtype\n",
        "def converting_float_to_int(df):\n",
        "    for column in df.columns:\n",
        "        if df[column].dtype == 'float64':\n",
        "            df[column] = df[column].astype(np.uint8)\n",
        "    return df\n",
        "churnTrain_df = converting_float_to_int(churnTrain_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4bSUTFWvMB2u",
        "outputId": "52ffa944-3496-4a43-d405-ee3f042fd88d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 440832 entries, 0 to 440832\n",
            "Data columns (total 11 columns):\n",
            " #   Column             Non-Null Count   Dtype\n",
            "---  ------             --------------   -----\n",
            " 0   Gender             440832 non-null  int64\n",
            " 1   Tenure             440832 non-null  uint8\n",
            " 2   Usage Frequency    440832 non-null  uint8\n",
            " 3   Support Calls      440832 non-null  uint8\n",
            " 4   Payment Delay      440832 non-null  uint8\n",
            " 5   Subscription Type  440832 non-null  int64\n",
            " 6   Contract Length    440832 non-null  int64\n",
            " 7   Last Interaction   440832 non-null  uint8\n",
            " 8   Churn              440832 non-null  uint8\n",
            " 9   Total Spend Bin    440832 non-null  int64\n",
            " 10  Age Bin            440832 non-null  int64\n",
            "dtypes: int64(5), uint8(6)\n",
            "memory usage: 38.8 MB\n"
          ]
        }
      ],
      "source": [
        "churnTrain_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "hy-J0mR_Bqa7",
        "outputId": "beb8a312-9cf7-4133-b6ac-9c3688cb6d33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 64374 entries, 0 to 64373\n",
            "Data columns (total 11 columns):\n",
            " #   Column             Non-Null Count  Dtype\n",
            "---  ------             --------------  -----\n",
            " 0   Gender             64374 non-null  int64\n",
            " 1   Tenure             64374 non-null  int64\n",
            " 2   Usage Frequency    64374 non-null  int64\n",
            " 3   Support Calls      64374 non-null  int64\n",
            " 4   Payment Delay      64374 non-null  int64\n",
            " 5   Subscription Type  64374 non-null  int64\n",
            " 6   Contract Length    64374 non-null  int64\n",
            " 7   Last Interaction   64374 non-null  int64\n",
            " 8   Churn              64374 non-null  int64\n",
            " 9   Total Spend Bin    64374 non-null  int64\n",
            " 10  Age Bin            64374 non-null  int64\n",
            "dtypes: int64(11)\n",
            "memory usage: 5.4 MB\n"
          ]
        }
      ],
      "source": [
        "churnTest_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-T8joRCg6yLB"
      },
      "source": [
        "## Implement Decision Tree Classifier\n",
        "This approach employs binary splits to efficiently streamline data processing by utilizing techniques like categorical binning and normalization. Based on the prior task, the Z-score normalization was found to be less beneficial for this decision tree model due to its binary splitting nature.\n",
        "\n",
        "<strong>The simplified general procedure can be summarized as follows:</strong>\n",
        "\n",
        "1. Determination of best split <br>\n",
        "   In this implementation, models will make splits based on various measures of impurity and uncertainty, including but not limited to entropy gain ratio, Gini index, and variance, depending on the specific requirements.\n",
        "\n",
        "2. Induction using the best splitting function.\n",
        "\n",
        "3. Repeat until we arrive at the leaf node or satisfy a stopping condition.\n",
        "\n",
        "### The followings functions have been implemented to fulfill our objectives:\n",
        "\n",
        "1. A function to compute entropy, Gini index, gain ratio, and information gain.\n",
        "2. A function to determine the optimal split using Gini index, gain ratio, or information gain as specified through a split criterion parameter.\n",
        "3. A function for prediction and scoring.\n",
        "4. Establish a Decision Tree (DT) class and instantiate three DT models with user-defined split criteria using the provided functions.\n",
        "\n",
        "### Decision trees as classifiers\n",
        "Employing data preparation techniques like categorical binning and normalization can substantially decrease computational overhead and streamline data processing in this approach. Notably, the decision tree model developed in the prior task does not benefit from z-score normalization, given its inherent binary splitting structure.\n",
        "\n",
        "Here's a streamlined overview of the general process:\n",
        "\n",
        "1. Selecting the Optimal Splitting Criterion:\n",
        "   There are different ways to check how mixed or uncertain our data is, like using variance, the Gini index, or the entropy gain ratio. In our setup, we'll split each model based on what works best for the task, which could be the Gini index, Gini ratio, or information gain.\n",
        "\n",
        "2. Using the Optimal Splitting Function for Induction.\n",
        "\n",
        "3. Repeating the process until reaching a stopping criterion or the leaf node.\n",
        "\n",
        "### To achieve our objectives, the following processes are employed:\n",
        "\n",
        "1. A function for calculating entropy, the Gini index, gain ratio, and information gain.\n",
        "\n",
        "2. A unified function for determining the optimal split, capable of handling the Gini index, gain ratio, and information gain based on the specified splitting criterion.\n",
        "\n",
        "3. A function for prediction and scoring.\n",
        "\n",
        "4. Implementing the decision tree model within a DT class using the newly created functions. Three instances of the DT model are generated based on the user's input for the split criterion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiBZYXwuLHaQ"
      },
      "source": [
        "#### Calculate Entropy function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ethokHN8LOs2"
      },
      "outputs": [],
      "source": [
        "def entropy(y):\n",
        "    _, counts = np.unique(y, return_counts=True)\n",
        "    probabilities = counts / len(y)\n",
        "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
        "    return entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QsGzzUALglT"
      },
      "source": [
        "#### Create information gain function \n",
        "\n",
        "info gain = H(Y) - P(left) * H(Y|left) - P(right) * H(Y|right)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "f6LSUYzQLjuL"
      },
      "outputs": [],
      "source": [
        "def information_gain(y_parent, y_left, y_right):\n",
        "    parent_entropy = entropy(y_parent)\n",
        "    left_entropy = entropy(y_left)\n",
        "    right_entropy = entropy(y_right)\n",
        "\n",
        "    parent_weight = len(y_parent) / (len(y_left) + len(y_right))\n",
        "    left_weight = len(y_left) / len(y_parent)\n",
        "    right_weight = len(y_right) / len(y_parent)\n",
        "\n",
        "    information_gain = parent_entropy - (parent_weight * left_entropy) - (parent_weight * right_entropy)\n",
        "    return information_gain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM4YQTbgLla-"
      },
      "source": [
        "#### Create gain_ratio function \n",
        "\n",
        "gain ratio = information gain / splitInfo <br>\n",
        "split info = -sum(P(subset) * log2(P(subset)))\n",
        "\n",
        "1. Compute the information gain and split information for every feature.\n",
        "2. Contrast the gain ratios for each feature.\n",
        "3. Choose the feature with the highest gain ratio as the one for splitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pL9qBIM-L0Cx"
      },
      "outputs": [],
      "source": [
        "def gain_ratio(y_parent, y_left, y_right):\n",
        "    info_gain = information_gain(y_parent, y_left, y_right)\n",
        "    parent_entropy = entropy(y_parent)\n",
        "\n",
        "    split_information = entropy(np.concatenate([y_left, y_right]))\n",
        "    if split_information == 0:\n",
        "        return 0  # To avoid division by zero\n",
        "\n",
        "    gain_ratio = info_gain / split_information\n",
        "    return gain_ratio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhtN5WqWL12M"
      },
      "source": [
        "#### Create gini index function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "r3JIrm9HL6IE"
      },
      "outputs": [],
      "source": [
        "def gini_index(y_parent, y_left, y_right):\n",
        "    parent_gini = gini(y_parent)\n",
        "    left_gini = gini(y_left)\n",
        "    right_gini = gini(y_right)\n",
        "\n",
        "    parent_weight = len(y_parent) / (len(y_left) + len(y_right))\n",
        "    left_weight = len(y_left) / len(y_parent)\n",
        "    right_weight = len(y_right) / len(y_parent)\n",
        "\n",
        "    gini_index = parent_gini - (parent_weight * left_gini) - (parent_weight * right_gini)\n",
        "    return gini_index\n",
        "\n",
        "def gini(y):\n",
        "    _, counts = np.unique(y, return_counts=True)\n",
        "    probabilities = counts / len(y)\n",
        "    gini = 1 - np.sum(probabilities ** 2)\n",
        "    return gini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI8iARCVNwZk"
      },
      "source": [
        "#### Create find_best_split function for info gain, gain ratio and gini index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1RK3kcyUOmfN"
      },
      "outputs": [],
      "source": [
        "def find_best_split(X, y, split_criterion='information_gain'):\n",
        "    best_split_feature = None\n",
        "    best_split_value = None\n",
        "    best_gain = -1\n",
        "\n",
        "    for feature_idx in range(X.shape[1]):\n",
        "        feature_values = X[:, feature_idx]\n",
        "        unique_values = np.unique(feature_values)\n",
        "        for value in unique_values:\n",
        "            left_indices = feature_values < value\n",
        "            right_indices = ~left_indices\n",
        "\n",
        "            if split_criterion == 'information_gain':\n",
        "                current_gain = information_gain(y, y[left_indices], y[right_indices])\n",
        "            elif split_criterion == 'gain_ratio':\n",
        "                current_gain = gain_ratio(y, y[left_indices], y[right_indices])\n",
        "            elif split_criterion == 'gini_index':\n",
        "                current_gain = gini_index(y, y[left_indices], y[right_indices])\n",
        "            else:\n",
        "                raise ValueError(\"Invalid split criterion. Supported options are: 'information_gain', 'gain_ratio', and 'gini_index'.\")\n",
        "\n",
        "            if current_gain > best_gain:\n",
        "                best_gain = current_gain\n",
        "                best_split_feature = feature_idx\n",
        "                best_split_value = value\n",
        "\n",
        "    return best_split_feature, best_split_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQV8MOTDOuxr"
      },
      "source": [
        "#### Create class to implement all the functions to for a tree and predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "PLdDM4tsO5-X"
      },
      "outputs": [],
      "source": [
        "class DecisionTree:\n",
        "    def __init__(self, split_criterion='information_gain', max_depth=None, min_samples_split=2):\n",
        "        self.split_criterion = split_criterion\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.tree = None\n",
        "\n",
        "    def fit(self, X, y, depth=0):\n",
        "        self.tree = self._build_tree(X, y, depth)\n",
        "\n",
        "    def predict(self, X):\n",
        "        if self.tree is None:\n",
        "            raise ValueError(\"Model has not been trained yet. Call 'fit' before 'predict'.\")\n",
        "\n",
        "        return np.array([self._predict_sample(sample, self.tree) for sample in X])\n",
        "\n",
        "    def _build_tree(self, X, y, depth):\n",
        "        if len(np.unique(y)) == 1:\n",
        "            # If all labels are the same, return a leaf node with the label\n",
        "            return np.unique(y)[0]\n",
        "\n",
        "        if self.max_depth is not None and depth >= self.max_depth:\n",
        "            # If the maximum depth is reached, return the most common label\n",
        "            return np.bincount(y).argmax()\n",
        "\n",
        "        if X.shape[0] < self.min_samples_split:\n",
        "            # If the number of samples is below the minimum required for splitting, return the most common label\n",
        "            return np.bincount(y).argmax()\n",
        "\n",
        "        if X.shape[1] == 0:\n",
        "            # If there are no features left, return the most common label\n",
        "            return np.bincount(y).argmax()\n",
        "\n",
        "        best_split_feature, best_split_value = find_best_split(X, y, self.split_criterion)\n",
        "        left_indices = X[:, best_split_feature] < best_split_value\n",
        "        right_indices = ~left_indices\n",
        "\n",
        "        # Check if left or right subtree has no samples\n",
        "        if np.all(left_indices) or np.all(right_indices):\n",
        "            return np.bincount(y).argmax()\n",
        "\n",
        "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
        "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
        "\n",
        "        return (best_split_feature, best_split_value, left_subtree, right_subtree)\n",
        "\n",
        "    def _predict_sample(self, sample, node):\n",
        "        if not isinstance(node, tuple):\n",
        "            return node\n",
        "\n",
        "        split_feature, split_value, left_subtree, right_subtree = node\n",
        "        if sample[split_feature] < split_value:\n",
        "            return self._predict_sample(sample, left_subtree)\n",
        "        else:\n",
        "            return self._predict_sample(sample, right_subtree)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2IcsO3CAbVK"
      },
      "source": [
        "Create the ensemble model.\n",
        "\n",
        "We merge three DT models—Info Gain, Gini Index, and Gain Ratio—constructed using diverse splitting criteria, which may lead to longer processing times but improved accuracy. \n",
        "\n",
        "The ensemble model will employ a simple voting method to decide the final prediction's outcome.\n",
        "\n",
        "The ultimate forecast from the ensemble model will be determined by majority vote. For instance, if two DT models and one DT model all predict that an individual will churn (churn 1), the final prediction will also be churn 1 due to the majority consensus.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Sji0oQnVBOZp"
      },
      "outputs": [],
      "source": [
        "class DecisionTreeEnsemble:\n",
        "    def __init__(self, models):\n",
        "        self.models = models\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for model in self.models:\n",
        "            model.fit(X, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = [model.predict(X) for model in self.models]\n",
        "        return self._majority_vote(predictions)\n",
        "\n",
        "    def _majority_vote(self, predictions):\n",
        "        final_predictions = np.zeros(predictions[0].shape)\n",
        "        for pred in predictions:\n",
        "            final_predictions += pred\n",
        "\n",
        "        final_predictions = (final_predictions >= (len(self.models) / 2)).astype(int)\n",
        "        return final_predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlkJ7wMjCqva"
      },
      "source": [
        "#### Evaluation function "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ZMIFihyyCv4N"
      },
      "outputs": [],
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "    return np.mean(y_true == y_pred)\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    true_positive = np.sum((y_true == 1) & (y_pred == 1))\n",
        "    false_positive = np.sum((y_true == 0) & (y_pred == 1))\n",
        "    return true_positive / (true_positive + false_positive)\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    true_positive = np.sum((y_true == 1) & (y_pred == 1))\n",
        "    false_negative = np.sum((y_true == 1) & (y_pred == 0))\n",
        "    return true_positive / (true_positive + false_negative)\n",
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "    prec = precision(y_true, y_pred)\n",
        "    rec = recall(y_true, y_pred)\n",
        "    return 2 * (prec * rec) / (prec + rec)\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    acc = accuracy(y_test, y_pred)\n",
        "    prec = precision(y_test, y_pred)\n",
        "    rec = recall(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    return acc, prec, rec, f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdMPEkS7YSkc"
      },
      "source": [
        "#### Implementation of random forest models created"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6ZiHANzYa9T"
      },
      "source": [
        "##### Step 1: Extract features and target variable from DataFrames\n",
        "1.Implementation assumes data is preprocessed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "nurt4VKvYWcP"
      },
      "outputs": [],
      "source": [
        "# Convert the DataFrame to numpy arrays\n",
        "X_train = churnTrain_df.drop('Churn', axis=1)\n",
        "y_train = churnTrain_df['Churn']\n",
        "X_test = churnTest_df.drop('Churn', axis=1)\n",
        "y_test = churnTest_df['Churn']\n",
        "\n",
        "X_train = churnTrain_df.values\n",
        "y_train = churnTrain_df.values\n",
        "X_test = churnTest_df.values\n",
        "y_test = churnTest_df.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpOw9mb_YonM"
      },
      "source": [
        "Step 2: Create and train each of the three Decision Tree models using a unique split criteria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "M7sbqBCcYt_Q"
      },
      "outputs": [],
      "source": [
        "# Create Decision Tree models\n",
        "dt_info_gain = DecisionTree(split_criterion='information_gain', max_depth=2, min_samples_split=6)\n",
        "dt_gain_ratio = DecisionTree(split_criterion='gain_ratio', max_depth=2, min_samples_split=6)\n",
        "dt_gini_index = DecisionTree(split_criterion='gini_index', max_depth=2, min_samples_split=6)\n",
        "ensemble = DecisionTreeEnsemble(models=[dt_info_gain, dt_gain_ratio, dt_gini_index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "9I97V_aeEc85"
      },
      "outputs": [],
      "source": [
        "dt_info_gain.fit(X_train, y_train)\n",
        "dt_gain_ratio.fit(X_train, y_train)\n",
        "dt_gini_index.fit(X_train, y_train)\n",
        "ensemble.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HTDS8txEdiv",
        "outputId": "3c49cd50-19bb-4619-c95f-2911873cf4d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics for Information Gain DT:\n",
            "Accuracy: 0.58\n",
            "Precision: 0.54\n",
            "Recall: 0.78\n",
            "F1-score: 0.64\n",
            "\n",
            "Metrics for Gain Ratio DT:\n",
            "Accuracy: 0.58\n",
            "Precision: 0.54\n",
            "Recall: 0.78\n",
            "F1-score: 0.64\n",
            "\n",
            "Metrics for Gini Index DT:\n",
            "Accuracy: 0.60\n",
            "Precision: 0.55\n",
            "Recall: 0.86\n",
            "F1-score: 0.67\n",
            "\n",
            "Metrics for Ensemble:\n",
            "Accuracy: 0.58\n",
            "Precision: 0.54\n",
            "Recall: 0.78\n",
            "F1-score: 0.64\n",
            "\n"
          ]
        }
      ],
      "source": [
        "models = [dt_info_gain, dt_gain_ratio, dt_gini_index, ensemble]\n",
        "model_names = [\"Information Gain DT\", \"Gain Ratio DT\", \"Gini Index DT\", \"Ensemble\"]\n",
        "\n",
        "for model, name in zip(models, model_names):\n",
        "    acc, prec, rec, f1 = evaluate_model(model, X_test, y_test)\n",
        "    print(f\"Metrics for {name}:\")\n",
        "    print(f\"Accuracy: {acc:.2f}\")\n",
        "    print(f\"Precision: {prec:.2f}\")\n",
        "    print(f\"Recall: {rec:.2f}\")\n",
        "    print(f\"F1-score: {f1:.2f}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e113oq4P0YN",
        "outputId": "dcecf751-cb9c-4fbc-cb62-64d364f6d5c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics for Information Gain DT:\n",
            "Accuracy: 0.81\n",
            "Precision: 1.00\n",
            "Recall: 0.67\n",
            "F1-score: 0.80\n",
            "\n",
            "Metrics for Gain Ratio DT:\n",
            "Accuracy: 0.81\n",
            "Precision: 1.00\n",
            "Recall: 0.67\n",
            "F1-score: 0.80\n",
            "\n",
            "Metrics for Gini Index DT:\n",
            "Accuracy: 0.84\n",
            "Precision: 0.99\n",
            "Recall: 0.73\n",
            "F1-score: 0.84\n",
            "\n",
            "Metrics for Ensemble:\n",
            "Accuracy: 0.81\n",
            "Precision: 1.00\n",
            "Recall: 0.67\n",
            "F1-score: 0.80\n",
            "\n"
          ]
        }
      ],
      "source": [
        "models = [dt_info_gain, dt_gain_ratio, dt_gini_index, ensemble]\n",
        "model_names = [\"Information Gain DT\", \"Gain Ratio DT\", \"Gini Index DT\", \"Ensemble\"]\n",
        "\n",
        "for model, name in zip(models, model_names):\n",
        "    acc, prec, rec, f1 = evaluate_model(model, X_train, y_train)\n",
        "    print(f\"Metrics for {name}:\")\n",
        "    print(f\"Accuracy: {acc:.2f}\")\n",
        "    print(f\"Precision: {prec:.2f}\")\n",
        "    print(f\"Recall: {rec:.2f}\")\n",
        "    print(f\"F1-score: {f1:.2f}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0MMpta7C7Py"
      },
      "source": [
        "## Implementing a random forest\n",
        "\n",
        "I'm using Bagging, a standard technique in Random Forests. It involves training multiple decision trees on random data subsets, and the final prediction is based on averaging (for regression) or majority vote (for classification) from these trees.\n",
        "\n",
        "The dataset is relatively straightforward and has undergone comprehensive cleaning. Employing a random forest in this context may not yield significant benefits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lpAS5mOih5J"
      },
      "outputs": [],
      "source": [
        "class RandomForestFromScratch:\n",
        "    def __init__(self, decision_tree, n_estimators=100, max_samples=None, max_depth=None, min_samples_split=2):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_samples = max_samples\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.decision_tree = decision_tree\n",
        "        self.estimators = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for _ in range(self.n_estimators):\n",
        "            estimator = self.decision_tree  # Use the provided Decision Tree instance\n",
        "            if self.max_samples is not None:\n",
        "                bootstrap_indices = np.random.choice(len(X), size=self.max_samples, replace=True)\n",
        "                X_bootstrap = X[bootstrap_indices]\n",
        "                y_bootstrap = y[bootstrap_indices]\n",
        "                estimator.fit(X_bootstrap, y_bootstrap)\n",
        "            else:\n",
        "                estimator.fit(X, y)\n",
        "\n",
        "            self.estimators.append(estimator)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = np.array([estimator.predict(X) for estimator in self.estimators])\n",
        "        return np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=predictions)\n",
        "\n",
        "\n",
        "# Create a RandomForest classifier based on the Decision Tree model using Information Gain\n",
        "rf_gini_index = RandomForestFromScratch(decision_tree=dt_gini_index, n_estimators=5, max_depth=2, min_samples_split=6)\n",
        "\n",
        "# Fit the model\n",
        "rf_gini_index.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_rf_gini_index = rf_gini_index.predict(X_test)\n",
        "\n",
        "# Evaluate the model using the functions provided\n",
        "accuracy_rf_gini_index, precision_rf_gini_index, recall_rf_gini_index, f1_rf_gini_index = evaluate_model(rf_gini_index, X_test, y_test)\n",
        "\n",
        "# Print evaluation results\n",
        "print(\"Random Forest Classifier (Gini index):\")\n",
        "print(f\"Accuracy: {accuracy_rf_gini_index:.4f}\")\n",
        "print(f\"Precision: {precision_rf_gini_index:.4f}\")\n",
        "print(f\"Recall: {recall_rf_gini_index:.4f}\")\n",
        "print(f\"F1-Score: {f1_rf_gini_index:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "SXX3j3ly36ew",
        "BEEynKr84g2u"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
